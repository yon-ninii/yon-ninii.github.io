{
    "componentChunkName": "component---src-templates-blog-template-js",
    "path": "/VGGnet/",
    "result": {"data":{"cur":{"id":"275a3a76-e646-5355-b9a5-bbd0502c3614","html":"<p><strong>안녕하세요. 오늘은 드디어 첫번째 논문 리뷰 포스팅입니다! 오늘 리뷰할 논문은 바로 2014년 옥스포드 대학의 연구팀 VGG에 의해 발표된 “Very Deep Convolutional Networks for Large-Scale Image Recognition” 입니다!</strong></p>\n<p><a href=\"https://arxiv.org/abs/1409.1556\">https://arxiv.org/abs/1409.1556</a></p>\n<p><strong>당시 이미지넷 인식 대회에서 준우승을 한 획기적인 네트워크였습니다. VGGNet은 VGG-16과 VGG-19로 나뉘는데 이 숫자는 Layer의 개수를 의미합니다. 한 마디로 16계층과 19계층짜리 네트워크를 만든거라고 보면 됩니다. 이 논문이 중요하다고 생각하는 이유는 해당 논문의 발표 이후로 네트워크의 깊이가 얼마나 중요한지, 또한 어떻게 깊게 만드는지에 대한 인식을 하기 시작했다고 생각하기 때문입니다.</strong></p>\n<p><strong>자, 이제 리뷰를 시작해보겠습니다.</strong></p>\n<h3 id=\"0-들어가기-전\" style=\"position:relative;\"><a href=\"#0-%EB%93%A4%EC%96%B4%EA%B0%80%EA%B8%B0-%EC%A0%84\" aria-label=\"0 들어가기 전 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>0. 들어가기 전..</h3>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 56.666666666666664%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsTAAALEwEAmpwYAAAB0klEQVQoz32S627TQBBG8/7vAkhI/EAFBKrUohYRR6Fx0jiJvZv6fontNL6uD7JdKiiEkdZjz2qOv539JlVV0TQNSinath2zaqnr+rl2binVAh2+7yOlJAxDJgCOY7Nc6azX9xjGGrmXmOaOxeIOS1i8jE4pQHE8FXhhMta6bhA26R99pKGHmN7iGUuaZlRXVeWg5FeDUt3wLrwDK+Fj+jlvb6ynff4E9g3VKaOpir8V9bC+gxE43cVcaA4iOPL6qxhq6iVwaKSjbSrOxd4/YPsx+j7l42wEvjkH7Lrx73VdkiQReZ6THhLK0xHLjcmPj9waEZ/mDoaTcaHZiOCRV9f/AaonJfebLUmasto9EEYxlwsXbRuiy4SrpcvWzfg8txF+xrtv8h8zfFLn313BY8T77xLpxnyYCuY7n6WxQ99YrDYmmr4hygq0e8nalCxkfOZSADn9wjGQzAx7sMPasrE9n614QNgu8sFmbe6xXQfdMHE9n5m+eb6sAaiebaGQtjMcVVhbhLDY7wWu53BIYrI0RQoLyzLJ82yYdVPX1L+5orfY5PJGQ/uhQdcOc+zBvd+iOCIIAqIoIgjHHCcxySEhjmOKoqAsS4qy5HQ6Dd99/gnOsEb9n3g3qgAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Untitled\"\n        title=\"Untitled\"\n        src=\"/static/a17b63ce85089208d871ff9ed011e461/37523/Untitled.png\"\n        srcset=\"/static/a17b63ce85089208d871ff9ed011e461/e9ff0/Untitled.png 180w,\n/static/a17b63ce85089208d871ff9ed011e461/f21e7/Untitled.png 360w,\n/static/a17b63ce85089208d871ff9ed011e461/37523/Untitled.png 720w,\n/static/a17b63ce85089208d871ff9ed011e461/fd28b/Untitled.png 811w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<ul>\n<li>\n<p>위 그림은 네트워크 깊이와 ImageNet 에러율에 대한 그래프입니다.</p>\n</li>\n<li>\n<p>VGG 이전에 발표된 AlexNet과 같은 네트워크들을 8개의 계층밖에 가지지 못하면서 에러율도 높다는 것을 확인할 수 있는데요, 반면에 VGG부터 Layer의 수가 늘어나면서 에러율 또한 현저히 낮아진 것을 알 수 있습니다.</p>\n</li>\n<li>\n<p>이 다음에 리뷰할 예정인 ResNet부터 Layer의 수가 폭발적으로 증가한 것 또한 확인할 수 있습니다.</p>\n</li>\n</ul>\n<h3 id=\"1-네트워크의-깊이depth\" style=\"position:relative;\"><a href=\"#1-%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC%EC%9D%98-%EA%B9%8A%EC%9D%B4depth\" aria-label=\"1 네트워크의 깊이depth permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>1. 네트워크의 깊이(Depth)</h3>\n<ul>\n<li>\n<p>그러면 왜 그동안 네트워크의 깊이를 마냥 늘려가지 못했을까요? 이 의문에 대한 답은 <strong>Gradient Vanishing, Overfitting, Increasing train time</strong> 등의 이유가 있습니다.</p>\n</li>\n<li>\n<p><strong>Gradient Vanishing</strong>은  활성화 함수(Activation function)을 이용해서 기울기 기반 학습(Gradient based training)을 진행할 때 발생하는 문제입니다. 간단하게 설명하자면 역전파를 이용해서 학습시킬 때 파라미터 값의 변화량인 Gradient가 너무 적게 변화한다면 training이 정상적으로 수행되지 않은 상태에서 수렴해버리는 문제가 발생하고 이를 Gradient가 사라진다고 해서 Gradient Vanishing이라고 합니다.</p>\n</li>\n<li>\n<p><strong>Overfitting</strong>은 과적합이라는 뜻인데, 훈련 데이터 셋으로 과하게 학습되어 모델의 파라미터들이 학습 데이터 셋에 너무 들어맞는(?) 상태가 되어 새로운 데이터 셋인 테스트 데이터가 들어오면 제대로 동작하지 못하는 문제입니다. 주로 모델이 너무 복잡할 때 발생합니다.</p>\n</li>\n<li>\n<p><strong>Increasing train time</strong>은 간단하게 학습 시간이 너무 오래 걸린다는 뜻입니다.</p>\n</li>\n<li>\n<p>이러한 문제점들이 항상 있었고 해당 논문의 저자는 이 한계들만 뛰어넘는다면 더욱 정확한 모델을 만들 수 있겠다는 생각이 들었다고 합니다.</p>\n</li>\n</ul>\n<h3 id=\"2-vggnet의-구조\" style=\"position:relative;\"><a href=\"#2-vggnet%EC%9D%98-%EA%B5%AC%EC%A1%B0\" aria-label=\"2 vggnet의 구조 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2. VGGNet의 구조</h3>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 63.33333333333333%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAAAsTAAALEwEAmpwYAAAB+klEQVQ4y52T22sTURDG8wcLoiKCQvHBJwWFUlCiPoiKl6I10sakLTYlXZsaamlNTHpJu203JtmQ3ewlu5u9/SRbtk2jJcWBc85wDvPN982ZSYRhyMD6/T5BEHAktYnvBmfsX9YSg61UKvPuzUtEOSS73vxvsFPA58+STNy9T3Y7pHhs4vcdgvAMdJjxWMCVlTw3rt9kcvo7+UOXjLBPSzGjx2AANsQ2GEowuuKEiWtXr3D73hOyuz45YYcZocHOoYqs9E5SBu6lpMagiYk7t0i+X2K+KDM994vlQo23mV1m8yLVap18qYUotum0DaqigtrpYVtdunYX3dZQTQXTNs4YTj56wIfFLdKrdWYyFRaz6zydl1jKbZFe2OTF8hE/Vn9SLOwyt9Zka+OAbueYY1Viv1njd1ei3pZwbOeEYfLxFKlchbQg8TFTZV6okRFEynstdE1HUXVcLyDEj+TbjoVpWhBcIPnLbIpPX0ukCw1efS6zeWSCb+N57kjA+PpFkqcmH5J8vUCq3OfbdpueYdDrWed6MQ5QNA9F7WEYOpqmYxgGlmX9/SlrFZXUhn5Oxb/6T9rbo1at0Gg0EUURWZbRNC2aslOGSqdN5UCNakbgRdJGwWJftWU0S6HvuNi2HbEzTRPHcc4AB47vuQS+h+/7F47duCmJJf8B0u3UofrXrXEAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Untitled\"\n        title=\"Untitled\"\n        src=\"/static/096973cca401812f3ce8da407885ca07/37523/Untitled1.png\"\n        srcset=\"/static/096973cca401812f3ce8da407885ca07/e9ff0/Untitled1.png 180w,\n/static/096973cca401812f3ce8da407885ca07/f21e7/Untitled1.png 360w,\n/static/096973cca401812f3ce8da407885ca07/37523/Untitled1.png 720w,\n/static/096973cca401812f3ce8da407885ca07/ae694/Untitled1.png 850w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<ul>\n<li>해당 그림은 VGG-16의 구조입니다.</li>\n<li>VGG팀은 깊이가 주는 영향력을 알기 위해 kernel은 3x3, stride는 1로 고정해서 깊지만 더욱 간단한 모델을 설계했습니다.</li>\n<li>해당 kernel로 convolution을 진행한 후에는 2x2 Max pooling을 2의 stride로 진행합니다.</li>\n<li>그렇다면 왜 3x3 convolution을 사용했을까요? 그 이유는 Receptive Field와 Parameter 수에 있습니다.</li>\n</ul>\n<h3 id=\"3-why-3x3-convolution\" style=\"position:relative;\"><a href=\"#3-why-3x3-convolution\" aria-label=\"3 why 3x3 convolution permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>3. Why 3x3 Convolution?</h3>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 75.55555555555556%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAYAAADkmO9VAAAACXBIWXMAAAsTAAALEwEAmpwYAAAC2ElEQVQ4y51TS09TURCec9+P3tvSCwJGNBrUAIkQBW0RYqIGEEpVKpTykLcV2zQgLwVBTcqzUIWiKJAQY5CIsHTtjo0LYmJYuDKu/BfXzPXUxA0YJzmZe8+c+c7MfN+BytBSrGZg/Vl5MNEHAGkAYBDCoHcAtWyXTzlW5NOy3XVaxvF8CQB4VVFIMq5JHPwx/8jm8qPVna2bA+tjAGDHOBBWt8mCDgAqAEjVkdVw/cjWc9/QRtxdO+zCPZAcmuUBVFXkRPotQkX3i17/w3eD5cGFZgqoyyKPXp5rKqwY9+XkVPVv9g4sftpun/j41u1/UooVZh894lzudNVUFhxOBwAbxxLdKgYAUnADgXAxBOx2hcd2hR+Jmqbv897z+bdfX/X1rd3zRFaCRZ5wHlZTW5qr/1y61bPUfiETABRV5LTf3TFsGsfxKYRhUii4tSSetZvbbWB+7gF6oYJVY5LA83hG2p30YIznGOKgeRpIopqhyHqqIuuGKutOgZcMAHAiKa86XOz7SClhRE1VdafOS2oaw3BWR4rACubegyQxIm2Xsf5M00TH0goIx/IC3mzYeCuuGlkg2ZyEMIwMhPBJQk3zK1zOS4e/rNk7VNjhe3yu9cZwYbN3yO2/1nMK54eg5s6adYZnCfyzhQJTi/db5uciTfFYf1ti8U7t0xCyzXGCo+XSaS5aV4BoROAYyx8IGA7MxEZDK9MTg2/igx0vZ4N10S5kW5VklA4y6DBsokJnpNN57QPYEJsdC69Oz41uLCBgd/1EF00Wvkx5QruTVRcBgDukS076epT9ARtnZvtaF8YjjfFof1ti7q4/2pmUyV7Mm7M7WZVOdaZT4e8PWHK2+kxZcSC3vCSQW1bckOfKrziBibLA6eaHFvgWvw6pmojPyqCA7EFj1KhcFPp2VUIIJtqCV04yKKksQwFCqMYOMruWKhqOTEGRbDaRl1DcOHTUIQf/Yb8AY5aW5OLuy9UAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Untitled\"\n        title=\"Untitled\"\n        src=\"/static/ee307bb02692b98b367df445aefa581b/37523/Untitled2.png\"\n        srcset=\"/static/ee307bb02692b98b367df445aefa581b/e9ff0/Untitled2.png 180w,\n/static/ee307bb02692b98b367df445aefa581b/f21e7/Untitled2.png 360w,\n/static/ee307bb02692b98b367df445aefa581b/37523/Untitled2.png 720w,\n/static/ee307bb02692b98b367df445aefa581b/302a4/Untitled2.png 1080w,\n/static/ee307bb02692b98b367df445aefa581b/21b4d/Untitled2.png 1280w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<ul>\n<li>\n<p>그림을 보면 쉽게 설명이 될 것 같습니다. 위 그림과 같이 7x7 입력 이미지에 3x3 convolution을 두번 진행하면 Receptive field는 5x5가 됩니다. 또한 아래 그림과 같이 5x5 convolution을 한번 진행하면 똑같이 5x5의 Receptive field를 가지게 됩니다. 하지만 이때 필요한 parameter 수가 바뀌는데요, 3x3을 두번 진행하면 9 + 9로 18개의 parameter가 필요하고 5x5는 25개의 parameter가 필요하게 됩니다. 즉, 같은 receptive field를 갖지만 적은 parameter가 쓰이는 네트워크를 만들 수 있다는 뜻입니다.</p>\n</li>\n<li>\n<p>고로 3x3 convolution만을 사용해서 네트워크를 설계하면 엄청나게 적어진 parameter 수로 앞서 설명한 문제점들이 줄어들었다는게 저자의 설명입니다.</p>\n</li>\n</ul>\n<h3 id=\"4-activation-function\" style=\"position:relative;\"><a href=\"#4-activation-function\" aria-label=\"4 activation function permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>4. Activation Function</h3>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 333px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 45.55555555555556%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAAAsTAAALEwEAmpwYAAAA+UlEQVQoz41Ra0/DMAzs//99CCS+MWBjYlMfZGni2L7JSVMy0B4nWRe78eVcd7gCVc1hCJFAidc68PtNVOFJwVLyDjdQm0yMRZp6YRN53QsGr4jpjmDr0PkZlFLOi5PCLzuGC3rR85DgHAnMsjhTSBYTuCDr2PVu1zZW/u8wLP9QkVjxvGWcYnEqTa8dV8FrkR0GgorApjVnP4uzPP6f+zeXYg4MiSICcRbzsW75cqKKbnMU7CfB6BW917yxGpaPM/A9JTx9ECbPgErZOgvSwiU4593XKCih2PaMzYHw2TPej4TdILn2diAMLoKIECjBnWbEhu0BopTPZywpxWqHo/RZAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Untitled\"\n        title=\"Untitled\"\n        src=\"/static/c9d2f71e8adb2ca57fa907f86a03d3d1/24c7e/Untitled3.png\"\n        srcset=\"/static/c9d2f71e8adb2ca57fa907f86a03d3d1/e9ff0/Untitled3.png 180w,\n/static/c9d2f71e8adb2ca57fa907f86a03d3d1/24c7e/Untitled3.png 333w\"\n        sizes=\"(max-width: 333px) 100vw, 333px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<ul>\n<li>\n<p>3x3 Convolution을 사용하는 또 하나의 장점은 convolution layer가 늘어남으로써 Activation Function을 더욱 여러번 수행할 수 있었습니다.</p>\n</li>\n<li>\n<p>네트워크를 깊게 만드는 데에서 <strong>비선형성(Non-linearity)</strong> 는 중요한 요소이기에 non-linear activation function인 <strong>ReLU</strong>를 사용해서 비선형성을 늘려줬다고 합니다.</p>\n</li>\n<li>\n<p>이렇게 비선형성을 늘려주면 모델은 더욱 유용한 특성을 뽑게 되고 학습의 효과가 증폭된다고 합니다.</p>\n</li>\n</ul>\n<h3 id=\"5-result\" style=\"position:relative;\"><a href=\"#5-result\" aria-label=\"5 result permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>5. Result</h3>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 102.22222222222221%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAACXBIWXMAAAsTAAALEwEAmpwYAAACe0lEQVQ4y33Uh25iQQwF0Pf/P4UUIdIghTRKQjrpBVJIIfHqOBoWrVZBssYz47Hvvfajur+/j4ODgxgOh3F7exubm5vRarXi8fExjo6OYnd3N25ubuLk5CT29vZydbe/vx+Hh4fx8PAQZ2dnMRgM8n11dXWVCSR1ubOzk0nu7u6i3+/HyspKBkq0vLycD4FYX1/PWL64tbW1uL6+/kko6OLiIh9Cyt/e3s5CGxsb0ev1YnV1Ner1eq6QlTiFLy8v4/z8PJlUKriwjsfjRNTtdpOWqgsLC3mPAVSFmmLisMKo3W5nXKWCgJKUbsyZygUdxGhDIs56enqaqLDkZ0KPwHdYLhVxtri4mIg1AAKmMZKRgoYSFd2xqFBjEIHPl1By3YNQ0q2trdxLVhjZawQQzhMhHVBSHSW6Mb4R4h8fH6cvzgp9s9nMvaTQQUzf6vPzM6bTaby9vaV9fX3l/vX1NT4+PnLvR1fjAQCEVomg03WFxFQeSvT8/BxPT0/pO9Px9/f3GI1G+VAyZ8XEM9K4p6PEFSHLrNFBNwWgwldZUzzodDopvEbyoYQQMufeV/MjoynzfmkACRRdWlqaNacMueT0N6N0Tw396IWq3/f3d9IpPloevry8pASk4RfK0GKjeRUU9LExAjrFitjmTqehEKPDxsr8MQxQLp/jLKFLCWkBuoTlHwUiuqJl9dCnZw956X5qqJNGo1Cmlz06zlB2VlAohqbCzmhOy0aj8fdLgYA2rIyFc0mhFoyygrQr4zWZTGZfGcu/r/mZmjePPEBPR2u12qwhzDurWEUUc1+Vjv1rAqFQ1R+rkfktVkJoqzJz/zMB9CI4fX6LLfYH8bS/pUwHwDsAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Untitled\"\n        title=\"Untitled\"\n        src=\"/static/38fbef97213d5711084b6187bcb04d34/37523/Untitled4.png\"\n        srcset=\"/static/38fbef97213d5711084b6187bcb04d34/e9ff0/Untitled4.png 180w,\n/static/38fbef97213d5711084b6187bcb04d34/f21e7/Untitled4.png 360w,\n/static/38fbef97213d5711084b6187bcb04d34/37523/Untitled4.png 720w,\n/static/38fbef97213d5711084b6187bcb04d34/302a4/Untitled4.png 1080w,\n/static/38fbef97213d5711084b6187bcb04d34/05fb0/Untitled4.png 1138w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<ul>\n<li>위 그림이 해당 논문에서 저자가 실험을 진행한 모델들입니다. A~E까지 6개의 모델로 나눠서 진행했는데 여기서 D와 E가 현재 우리가 아는 VGG-16과 19입니다.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 15.555555555555555%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAADCAYAAACTWi8uAAAACXBIWXMAAAsTAAALEwEAmpwYAAAApklEQVQI1zWPywqFAAhE+/8/qm0FQdG7iN4FRQ9atWhV56JwBRF1ZhyN932ROM+TLMvYto2qqnAchzzPqeuaJEk0fd/X3TAMmkVR0LYtz/PwfR+iZfR9r+AgCBjHkeu6VFQITdMQhiGmaeoBEZNecGVZ0nUdURRxHAfTNHHfN0aapliWhW3b6sh1XTzPQ+bLsrCuK3Ecs+878zzrF2JCsH8zUoUjmB/qx9VtYZw5HgAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Untitled\"\n        title=\"Untitled\"\n        src=\"/static/e8e1dd68e643f59eeea0efd68e6cd679/37523/Untitled5.png\"\n        srcset=\"/static/e8e1dd68e643f59eeea0efd68e6cd679/e9ff0/Untitled5.png 180w,\n/static/e8e1dd68e643f59eeea0efd68e6cd679/f21e7/Untitled5.png 360w,\n/static/e8e1dd68e643f59eeea0efd68e6cd679/37523/Untitled5.png 720w,\n/static/e8e1dd68e643f59eeea0efd68e6cd679/f79fa/Untitled5.png 940w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<ul>\n<li>위 표는 해당 모델들에서 사용된 Parameter의 수입니다. Layer의 수는 증가되었지만 parameter는 크게 증가되지 않은 점을 확인할 수 있습니다.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 44.44444444444444%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAAAsTAAALEwEAmpwYAAABd0lEQVQoz02S6Y6CQBCEef+30sguHtFIvFCjeIGo8RbBqzZfJ2z8MZkJ01X9dQ3Ofr9XkiRij6JI3o+rMBjo1/NUqVRUKpVULpdVr9fVbrfleZ5c11WtVlOj0dB2u9XxeNRut7PdieNYm83GzKI4VhJHOiSJ2r5vJoPBQNVqVePx2MQsagFAB8xqtVLh44xGI81mMz0eD2VZpizPld7vWi6XZna73dTtdk2Q57mZYU794XCwPQgCnU4npWkqJwxDw71er7pcLv/7fD63Ec/ns5rNphaLhZlPJhP1+30TQ0t9r9ezM7UO4s/no+fzaev1ehkpIowYyfd9I4YCAIjQcYfJcDg0Q4gdgmSUYjEC3SmCkFw6nY4RUgs5zTAqDImGsxnSFcL3+22roCXbVqtlRYy0Xq+NnAwhpDEN+AY1xETi8FpcfhNShIhfg3teGzIE0+nUyL8zZALOwFmGGNkLZ5mdETIyZLwuJhAiYPQiw+IhmIZo+A//AHZQm8Px+To+AAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Untitled\"\n        title=\"Untitled\"\n        src=\"/static/380d3159a4be8325c2f560f292c4db54/37523/Untitled6.png\"\n        srcset=\"/static/380d3159a4be8325c2f560f292c4db54/e9ff0/Untitled6.png 180w,\n/static/380d3159a4be8325c2f560f292c4db54/f21e7/Untitled6.png 360w,\n/static/380d3159a4be8325c2f560f292c4db54/37523/Untitled6.png 720w,\n/static/380d3159a4be8325c2f560f292c4db54/302a4/Untitled6.png 1080w,\n/static/380d3159a4be8325c2f560f292c4db54/37048/Untitled6.png 1352w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<ul>\n<li>하나의 Image scale만으로 테스트를 진행한 결과입니다.</li>\n<li>Error rate면에서 D와 E 모델이 나머지 모델들보다 우수한 것을 확인할 수 있습니다.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 36.11111111111111%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAYAAAAIy204AAAACXBIWXMAAAsTAAALEwEAmpwYAAABOUlEQVQoz1WR54rCABCE8/7v5B8rFmyXYiKKnphYYxJF7HN8e3hygWFJdmZ2duOkaarFYqE4jhUnib66XX1HkVzPV7lcVqvVUqVSURiGBt5d11W/39d4PNZms9F6vVaSJMqyTM5yuRSmbxR5rixNFYxGCoJA8/lcjUZDg8FARVEY0JxOJzMABDocDr+G+/1et9tNj8dD9/vdADkMR5rNZpaChPV6XbvdTqvVSr7vK89zbbdbGxBF0ceQxvV6/QPmNIbDoa0IuVqtqtlsmiErkhwOhlSGYggcPjyfT71eL6s85/PZRNPpVJPJRLVaTe1229KwEfeDg5bK0OPxaHC42+Vy+ZcQIQkhkrJUKtnaJOAHdDodE6PlPAxnU+AgxoTbvW9JAxLpSNnr9eR5nhlgyF+GQ1oqPHoY/gB0MgT1KSe1HQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Untitled\"\n        title=\"Untitled\"\n        src=\"/static/e9119b90d6146c0f28d9d236aa84694d/37523/Untitled7.png\"\n        srcset=\"/static/e9119b90d6146c0f28d9d236aa84694d/e9ff0/Untitled7.png 180w,\n/static/e9119b90d6146c0f28d9d236aa84694d/f21e7/Untitled7.png 360w,\n/static/e9119b90d6146c0f28d9d236aa84694d/37523/Untitled7.png 720w,\n/static/e9119b90d6146c0f28d9d236aa84694d/302a4/Untitled7.png 1080w,\n/static/e9119b90d6146c0f28d9d236aa84694d/85e74/Untitled7.png 1436w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<ul>\n<li>다음은 여러 scale에서 진행한 테스트 결과입니다.</li>\n<li>마찬가지로 D와 E 모델이 나머지 모델들보다 훨씬 우수하다는 것을 확인할 수 있습니다.</li>\n</ul>\n<h3 id=\"6-마무리\" style=\"position:relative;\"><a href=\"#6-%EB%A7%88%EB%AC%B4%EB%A6%AC\" aria-label=\"6 마무리 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>6. 마무리</h3>\n<p>해당 VGG 논문은 섣불리 늘리지 못했던 네트워크의 깊이를 늘린 것과 그에 따른 정확도의 향상의 상관관계를 널리 알린 연구로 큰 의미가 있다고 생각합니다. 이 다음엔 ResNet에 대한 논문을 리뷰해볼 예정인데 폭발적으로 증가한 Layer의 개수는 또 어떤 방법을 사용해서 늘린건지 한번 정리를 해보도록 하겠습니다 😄</p>\n<p>읽어주셔서 감사합니다!</p>\n<div class=\"table-of-contents\">\n<ul>\n<li><a href=\"#0-%EB%93%A4%EC%96%B4%EA%B0%80%EA%B8%B0-%EC%A0%84\">0. 들어가기 전..</a></li>\n<li><a href=\"#1-%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC%EC%9D%98-%EA%B9%8A%EC%9D%B4depth\">1. 네트워크의 깊이(Depth)</a></li>\n<li><a href=\"#2-vggnet%EC%9D%98-%EA%B5%AC%EC%A1%B0\">2. VGGNet의 구조</a></li>\n<li><a href=\"#3-why-3x3-convolution\">3. Why 3x3 Convolution?</a></li>\n<li><a href=\"#4-activation-function\">4. Activation Function</a></li>\n<li><a href=\"#5-result\">5. Result</a></li>\n<li><a href=\"#6-%EB%A7%88%EB%AC%B4%EB%A6%AC\">6. 마무리</a></li>\n</ul>\n</div>","excerpt":"안녕하세요. 오늘은 드디어 첫번째 논문 리뷰 포스팅입니다! 오늘 리뷰할 논문은 바로 2014년 옥스포드 대학의 연구팀 VGG에 의해 발표된 “Very Deep Convolutional Networks for Large-Scale Image Recognition” 입니다! https://arxiv.org/abs/1409.1556 당시 이미지넷 인식 대회에서 준우승을 한 획기적인 네트워크였습니다. VGGNet은 VGG-16과 VGG-19로 나뉘는데 이 숫자는 Layer의 개수를 의미합니다. 한 마디로 16계층과 19계층짜리 네트워크를 만든거라고 보면 됩니다. 이 논문이 중요하다고 생각하는 이유는 해당 논문의 발표 이후로 네트워크의 깊이가 얼마나 중요한지, 또한 어떻게 깊게 만드는지에 대한 인식을 하기 시작했다고 생각하기 때문입니다. 자, 이제 리뷰를 시작해보겠습니다. 0. 들어가기 전..  위 그림은 네트워크 깊이와 ImageNet 에러율에 대한 그래프입니다. VGG 이전에 발표된 A…","frontmatter":{"date":"January 22, 2022","title":"<논문리뷰> VGGNet","categories":"Papers","author":"Yon_ninii","emoji":"🧚🏻‍♀️"},"fields":{"slug":"/VGGnet/"}},"next":{"id":"abd2b81f-f373-51d0-97e9-3bb0172d8cf4","html":"<p><strong>안녕하세요. 오늘은 딥 러닝의 기초적인 이론들을 정리해보려고 합니다. 저는 뉴럴 네트워크 중에서도 CNN을 위주로 공부했기 때문에 아마 다른 부분을 다룬 포스팅을 원하신다면 다른 블로그를… 찾아보셔야 할거에요… 😂 그럼 시작해보겠습니다!</strong></p>\n<h3 id=\"0-들어가기-전\" style=\"position:relative;\"><a href=\"#0-%EB%93%A4%EC%96%B4%EA%B0%80%EA%B8%B0-%EC%A0%84\" aria-label=\"0 들어가기 전 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>0. 들어가기 전..</h3>\n<ul>\n<li>오늘 알아볼 것은 인공지능(A.I)에서도 머신러닝의 한 종류라고 할 수 있는 <strong>딥러닝</strong>입니다. 먼저 머신러닝(Machine Learning)은 해석해보면 ‘기계 학습’입니다. 처음 접하면 잘 이해가 안될 수 있지만 사실은 꽤나 직관적으로 표현된 이름입니다.</li>\n<li>기계를 학습시킨다.. 라는 뜻을 이해하려면 먼저 <strong>학습</strong>을 알아야 합니다. 여기서 쓰이는 학습이라는 의미는 기계(모델)가 스스로 데이터의 규칙(Solution)을 찾도록 하는 것입니다. 이 기술과 반대되는 기술은 전통적인 방법으로 ‘Traditional’, ‘Hand-crafted’라고 표현합니다. 다른 말로 사람이 직접 데이터의 규칙을 정하고 컴퓨터는 그저 연산만 진행하는 방법입니다. 하지만 머신러닝은 컴퓨터가 학습을 반복하면서 규칙을 찾고 최적의 해를 구한다는 점에서 굉장히 차이가 있다고 할 수 있습니다. 여기서 왜 ‘머신’ + ‘러닝’(기계 + 학습)이 직관적이라고 표현 했는지 이해가 될 것이라고 생각합니다.</li>\n<li>자, 그렇다면 이 머신러닝은 어떻게 구현이 될까 의문이 듭니다. 바로 여러가지 종류의 알고리즘 중에서 선택하면 되는데 그 중에서 인공신경망(Artificial Neural Network)을 쓰는 기술을 딥러닝이라고 합니다. 주로 머신러닝은 정형데이터, 딥러닝은 비정형데이터를 잘 처리한다고 하니 그 점 또한 참고해두면 좋을 것 같습니다. 이제 더 자세한 기초 이론을 뜯어보겠습니다!</li>\n</ul>\n<h3 id=\"1-뉴런\" style=\"position:relative;\"><a href=\"#1-%EB%89%B4%EB%9F%B0\" aria-label=\"1 뉴런 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>1. 뉴런</h3>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 79.44444444444444%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAQCAYAAAAWGF8bAAAACXBIWXMAAAsTAAALEwEAmpwYAAACjElEQVQ4yz2T2U6qQRCE54rEBRQQARHZkVUF2QRlDQQQw5IQE+HCAOGOhJfwAXjiPvk6+c9F54eZnuqq6m7j8/nE5XLJ9fW12O12ubq6EofDIaFQSMrlsnB/f38v8Xhc7zwez/+4u7vTs7OzM7HZbPo12WxWHh8fJRAIiNPpVDCS/H6/np+fnyso/wEAnLybmxvJ5XISi8Xk4uJC3xBmPB5LPp9XQIvl5eWluN1uSafTega7ZDIpiURCwuGwxsPDg6r4+PgQSFGQ/+bp6UkTLeqAwQBWr6+vCrTf7+V0OsnX15fUajVlDiiyO52O/P7+Srvdln6/LyYYDEokElEfAYQp1ZGWyWS0+mazkb+/P2m1Wnper9dVKoCw//7+lul0qmcGqgSMiGKxKNFoVM8o9PLyIpVKRc8JfJvNZlIqlSSVSukbFJFLGMzFJzxDOn4CQgAKuOXfer2Ww+Eg2+1W3t7elDGWcY9CpsIgi+5Bn2rPz88qA08Gg4GywDOYHY9Hlfb5+alAgFpBPiQMD+gOXuEdTKgIMME9yUwDD/GQe6uzvPF6vdJoNGQ4HIrhEpnQhTZNoTpMYIbRdJYcfjNK5ACCMmtGKd7r9cRgJEkkYC7fZrMpy+VSmQGEj1hD8PD9/V1Zkov/qCoUCtooc3t7q1PPHPIbCVQlGZmA4ikPGBdkLxYLmUwm+o6Vw3+IIduwakiwxgaWrBuXDC1gMOOLEhijYDQaSbfblWq1qsVoGvnqIcmwI5BAZYrgKZVJpjlIJ4fCyGPtkM97y2vDD3yhOt0mmZnEAgC5n8/nui3IZ+W4BwQwht2yja0zdIod3O122mmYYQNhjRFzt1qttOvsNwRg+PPzo8DIhhB2/QMCo9VLEAvdsgAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Untitled\"\n        title=\"Untitled\"\n        src=\"/static/bbe4fd6bda9f1ecc80845640e90ce60f/37523/Untitled.png\"\n        srcset=\"/static/bbe4fd6bda9f1ecc80845640e90ce60f/e9ff0/Untitled.png 180w,\n/static/bbe4fd6bda9f1ecc80845640e90ce60f/f21e7/Untitled.png 360w,\n/static/bbe4fd6bda9f1ecc80845640e90ce60f/37523/Untitled.png 720w,\n/static/bbe4fd6bda9f1ecc80845640e90ce60f/5a190/Untitled.png 800w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<ul>\n<li>모두들 알듯이 뇌에 존재하는 신경세포입니다. 생물학적으로 뉴런은 시냅스라는 뉴런 사이의 길을 통해서 전기신호를 이용해서 신호를 보냅니다. 이는 <strong>BNN</strong>, 즉 <strong>Biological Neural Network</strong>라고 합니다.</li>\n</ul>\n<h3 id=\"2-perceptron\" style=\"position:relative;\"><a href=\"#2-perceptron\" aria-label=\"2 perceptron permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2. Perceptron</h3>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 55.55555555555556%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsTAAALEwEAmpwYAAABEUlEQVQoz41T2W6DMBD0//9d+k7KVT+UUoI5bM6pZtONECG0K61so/Xs7Hgwt6pCUXxBI3iP6/Ud3ns5r+sq+d8w0zRhHEfM84xlWdC2Lay1CCE8AF+t22a6NwQhKJOgzjkkSYKmaU4vb2N7NgQZxztLgtZ1jTRNhekwDIcsGKxl0+DDM0OCaiE1zLIMfd/L2KrlZ1GIJBr8frm8wX7YZw3JhAyZVfUtj8LuPHOlpnmeo+s6eN/LyozjGGVZ/t6rhLXBTheyIDNlQ/YEjKJIwFVvSsJJCMhagpKYIZBe3luEexYdPYQ228ogI5/5TJkevfa+8c4288M2jXMy4t02y6FtzqwktlEwglPsM2P/9af8AJdoX/1yzMhHAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Untitled\"\n        title=\"Untitled\"\n        src=\"/static/0c5bd24d2351b02a861611efc2ffe87a/37523/Untitled1.png\"\n        srcset=\"/static/0c5bd24d2351b02a861611efc2ffe87a/e9ff0/Untitled1.png 180w,\n/static/0c5bd24d2351b02a861611efc2ffe87a/f21e7/Untitled1.png 360w,\n/static/0c5bd24d2351b02a861611efc2ffe87a/37523/Untitled1.png 720w,\n/static/0c5bd24d2351b02a861611efc2ffe87a/42d54/Untitled1.png 858w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<ul>\n<li>1957년 고안된 인공신경망입니다. 다수의 입력과 하나의 출력으로 나타내어지며 구성은 입력, 가중치, 활성함수, 그리고 출력으로 이루어져있습니다. 생물학적인 뉴런의 개념을 따서 만든 컴퓨팅 모델이라고 이해하시면 됩니다.</li>\n<li>이는 아래 그림과 같이 세가지 계층이 있는데 입력 계층, 은닉 계층, 그리고 출력 계층입니다.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 701px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 67.22222222222223%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAAAsTAAALEwEAmpwYAAAB/UlEQVQ4y32T627bMAyF/f5vtZ/FgKFr02JJFqeN76kdX3SxJNv5BmnLkKRACQiCBPKQ55CMzuczy7Lgb29d15EkCXmeM88zdV1zSN9I0gNCyODjfa/P5c9bxD+7fDRNQ5ZlAVApTa9adsUzaf0brdUnwPt3NAxDCPY2TRPzNF1yIIXiUOb8Sn6wy1e0/cC93YNGnmKgtyyB+qmTrN8asqOgaQeUMhinsG5EjwY1GjabNevNljiOgyzXFlVVxW63w7oZ6ybespaHp3eetkey8oS1juU8My8zzjmUNrRdzygHpJRYa0NlHrwsSyKfwTuGkpcFqTTVaaQVhuqj53j6YF++kBy3KK0wdqLN9nTxI+OpYDSWaZ6D7r6B0XW5Hnxy9v+7bgRJlbHaP7BJf9ILgRoddfxC+viNIY+R2oQkaZqGCYmEEKGz87wEwGMjWG0rdklLXrXocUTZHm0kzlmMsYh+YJEnJmcwxtw0KPK815sNznnqE4eiCxo+byvKjy5o+NebEKy0xVhPcwn6fWrK9Tz5Lo+j18+hzcwwSMq6Zl++ktUx/SDxrkpK+r4PTVFKhfsCfKOhB9Ra46zBWoNWmrIpeH3/HjQUSgQfz8qPmp+QoijC8aA3FV4P6GUVfVa/HU1f0okmVP+VBQ3vp/3ewW/P5Kag74XW/S5fx/8BrvLssBUfwxUAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Untitled\"\n        title=\"Untitled\"\n        src=\"/static/7e7813b1ece4ccf68a03e316aaccaace/49217/Untitled2.png\"\n        srcset=\"/static/7e7813b1ece4ccf68a03e316aaccaace/e9ff0/Untitled2.png 180w,\n/static/7e7813b1ece4ccf68a03e316aaccaace/f21e7/Untitled2.png 360w,\n/static/7e7813b1ece4ccf68a03e316aaccaace/49217/Untitled2.png 701w\"\n        sizes=\"(max-width: 701px) 100vw, 701px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<ul>\n<li>세개의 계층(Layer)중에서 왼쪽 계층인 입력 계층은 입력 데이터를 받는 계층이고 오른쪽 계층은 출력 계층으로 출력 결과물을 만들어내는 역할을 합니다. 여기서 중요한 부분은 입출력 계층 사이에 있는 <strong>은닉 계층(Hidden Layer)</strong> 입니다.</li>\n<li>위 그림에서 각 노드끼리 연결되어 있는 선들은 노드 사이에서 일어나는 연산을 의미합니다. 이때 연결 강도는 <strong>Weight(가중치)</strong> 라는 용어로 부르고 <strong>Bias(편향)</strong> 이라는 상수 또한 존재합니다.</li>\n<li>은닉 계층이 2개 이상인 네트워크를 <strong>Deep Neural Network</strong>라고 합니다.</li>\n</ul>\n<h3 id=\"3-weight가중치-bias편향\" style=\"position:relative;\"><a href=\"#3-weight%EA%B0%80%EC%A4%91%EC%B9%98-bias%ED%8E%B8%ED%96%A5\" aria-label=\"3 weight가중치 bias편향 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>3. Weight(가중치), Bias(편향)</h3>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 56.111111111111114%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsTAAALEwEAmpwYAAABkklEQVQoz62S30vbUBTH8/+/DYZM2IMwGHuYqzoZMpgrDato2lRnaGNrzRpTsY30h03SmubmM3JKbFcVfNgXDtzz5fs999xzrpamKVlkyM+rXAaVqn/yHKuceAAtJybh/RPDNJySxMljPp/PmUzuJbKzcEoRRFFedVHQsTx+7zv8qXeFD6MQ/aLE7nmBQm2bM+dUeNfto+td9JJH17sjRdEqGjR3itw6rmi0h3iG+fWC6geH4x2Lfr9P+bTM++omm0cbvC294ZP1kY7rUDNb7O1esf25QdW0OTkqY23tY737QrtokORP7tg3VPds3OaN3DKbzTi5PObA/sb3xgENtyH89bVP+VfWYQfP84Vr6hWswg98b+HVloN9OvR1KKWYTgOS5IE0VcuFrCxGClrnFqZZo92+IkkWS2hdtjBrJoc/D/G6nnBxHIuuXq9LPhwOMQyDSqWCbdvilSePRiN83ycIgscvE0URg8GAXq8nI8g7GI/Hos+3nuWZLgzDZYevwbruJZ/23Id+LtZ1L/k0/jP+AipTRxwAMQmAAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Untitled\"\n        title=\"Untitled\"\n        src=\"/static/d4c33eff3caa4c81f95916c8345e33ae/37523/Untitled3.png\"\n        srcset=\"/static/d4c33eff3caa4c81f95916c8345e33ae/e9ff0/Untitled3.png 180w,\n/static/d4c33eff3caa4c81f95916c8345e33ae/f21e7/Untitled3.png 360w,\n/static/d4c33eff3caa4c81f95916c8345e33ae/37523/Untitled3.png 720w,\n/static/d4c33eff3caa4c81f95916c8345e33ae/302a4/Untitled3.png 1080w,\n/static/d4c33eff3caa4c81f95916c8345e33ae/07a9c/Untitled3.png 1440w,\n/static/d4c33eff3caa4c81f95916c8345e33ae/29114/Untitled3.png 1920w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<ul>\n<li><strong>Weight(가중치)</strong> : Learnable Parameter로써 위에서 설명한 뉴런(노드) 사이의 연결 강도를 의미하며 이전 레이어에서 넘어온 입력이 다음 레이어에서 얼마나 중요한 역할을 하는지 결정해주는 변수라고 생각하시면 이해가 쉬울 것 같습니다. 중요도(가중치)를 의미하므로 입력 값에 곱해주게 됩니다. 훈련을 거듭하면서 더욱 정확한 결과물을 도출하기 위한 값으로 업데이트 됩니다.</li>\n<li><strong>Bias(편향)</strong> : 입력에 가중치를 곱해준 값에 더해주는 값으로 수식에서 상수와 같은 역할을 합니다. 이로써 $y = Wx + b$ 라는 수식이 완성됩니다. Bias 또한 훈련을 거듭하며 업데이트가 되며 활성화 함수를 거친 값에 최종적으로 출력 값을 조절해주는 역할을 합니다.</li>\n</ul>\n<h3 id=\"4-인공지능-부흥cnn\" style=\"position:relative;\"><a href=\"#4-%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5-%EB%B6%80%ED%9D%A5cnn\" aria-label=\"4 인공지능 부흥cnn permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>4. 인공지능 부흥(CNN)</h3>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 34.44444444444444%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAYAAAAIy204AAAACXBIWXMAAAsTAAALEwEAmpwYAAAA5UlEQVQoz32SyQrEQAhE+/8/MpBD9n3f4/AKHOY0DaK2VpVtEu77tvd97XkeefLruuTP85Q/jkN3HHKMAwbznBPKsrS2bWXjOFqWZdb3vTy1dV2taRorisK2bbOqqlRHbBgG5XVdS5SBQpIklqapCOZ5loeAJmoQAUQQga7rREIPMVj6yJk2eANAlIm5Aww5ZMRRFEkQAmxZlu/kiE/TpLWEPM9VoGHfdyMnpglySKhDzpMBMi339DIdHlGeHVCO41gkTMaUvhvAGEIsnhgQ+4PIicF8d8jSIaD5t+DT/Dv+h2D+lT8XeRjjECRL2wAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Untitled\"\n        title=\"Untitled\"\n        src=\"/static/5026df362afc32b514aaea4e6397d7d8/37523/Untitled4.png\"\n        srcset=\"/static/5026df362afc32b514aaea4e6397d7d8/e9ff0/Untitled4.png 180w,\n/static/5026df362afc32b514aaea4e6397d7d8/f21e7/Untitled4.png 360w,\n/static/5026df362afc32b514aaea4e6397d7d8/37523/Untitled4.png 720w,\n/static/5026df362afc32b514aaea4e6397d7d8/302a4/Untitled4.png 1080w,\n/static/5026df362afc32b514aaea4e6397d7d8/07a9c/Untitled4.png 1440w,\n/static/5026df362afc32b514aaea4e6397d7d8/71c1d/Untitled4.png 1536w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<ul>\n<li>\n<p>위 그림은 2012년 개최된 ILSVRC 대회에서 우승을 차지만 CNN 구조인 AlexNet입니다. CNN 구조의 부흥에 아주 큰 역할을 했다고 할 수 있습니다.</p>\n</li>\n<li>\n<p>먼저 인공지능 중에서도 CNN 구조의 딥 러닝에 대한 이론은 전부터 많이 나왔지만 제대로 쓰이지 못했던 가장 큰 이유는 앞서 소개한 Hand-crafted 모델에 비해 성능이 떨어졌기 때문입니다. 자, 그러면 왜 주목받지 못할 정도로 한참 성능이 떨어졌을까요?</p>\n<ol>\n<li>\n<p><strong>하드웨어의 한계</strong> : 엄청난 양의 파라미터들을 Handling하기에 병렬처리가 불가능했던 옛날 하드웨어의 성능이 따라가지 못했습니다.(현재는 엄청난 성능의 GPU로 모델의 학습 속도 향상 및 Real-time 연산 수행이 가능해짐)</p>\n</li>\n<li>\n<p><strong>알고리즘의 한계</strong> : 많이 연구되지 않은 분야였기에 알고리즘 적으로도 문제가 많았습니다.(최근 많은 주목을 받은 만큼 다양한 분야의 인재들이 대거 투입되어 활발한 연구 진행중. 매년 수 많은 연구가 진행되며 다양한 논문이 발표됨)</p>\n</li>\n<li>\n<p><strong>데이터의 한계</strong> : 모델 학습을 위해 수 많은 데이터들이 필요한데, 이 데이터들을 모두 수집하기에는 큰 어려움이 있었습니다.(현재는 인터넷의 발달로 많은 연구원들이 수집한 데이터셋들을 인터넷에서 쉽게 찾을 수 있음)</p>\n</li>\n</ol>\n</li>\n<li>\n<p>이렇게 많은 한계를 극복하고 2012년 AlexNet을 필두로 획기적인 전환점을 맞이합니다.</p>\n</li>\n</ul>\n<h3 id=\"5-신경망의-학습\" style=\"position:relative;\"><a href=\"#5-%EC%8B%A0%EA%B2%BD%EB%A7%9D%EC%9D%98-%ED%95%99%EC%8A%B5\" aria-label=\"5 신경망의 학습 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>5. 신경망의 학습</h3>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 513px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 81.66666666666667%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAQCAYAAAAWGF8bAAAACXBIWXMAAAsTAAALEwEAmpwYAAACOUlEQVQ4y3VUWeu5URA+5ep34WMqVxQXPoMvoBSSJCVxQym54YIiokTZ933ft/n3TI1e9J96O2fOmXlmeea8ijSy2Wyo2+1Sv9+nwWDA63g85rNGo0G9Xo9OpxPbvl4v/mQvoqBcr1daLBY0n895Bch0On2fjUYjms1mtFwu+Q77b9A34PF4ZKPdbscHl8uF1us1rVYr1pERgOR+MplQpVKh/X7/ASqfQgbBYJAzgjMCRKNRqtfrnDkAcI+sns8nPR4PBrrdbqx/i4rFYqSUokAgwAeFQoF1q9XKut/vZx0rwNrtNkUiEQ4s8pEhyvN4PNRsNrksZOrz+Sifz7NTq9Uir9fLpEDsdjsHSCaTtN1uuT3whR0CKlzAAGVBisUi/f39kc1mYz0UCvE91vv9TtVqlcLhMJ3P5x+GuWQgm0wmKpfLTAh6inLj8Tgb12o1MpvNVCqVWHc4HKTX6ymXy7H94XDgccJoQVeCjOggQNhE04fDIRMjmYDZRCJBRqOROp0OtwQgQiZIUsIaDjEqmD3RwawEABgcUqkUGQwGnk1JBL3Ehz3PYTab5VcBBzQ5k8m8SUCW6XSaS4K43W7S6XRsg5eFgLDBy0I1P2ODoYVusVg+SBHS4IR2/E8U+uZ0Orl/ELCHMQEwBGPkcrl4lRIFGBX9vBRBRunom4wDHMEgmi6BcI9eAgy91wK9M9S+XzQavcAYIGNkJWfY482DNO1f5mcOtQdgGQACiD0AsAcpAEbW3wOtzfAf35iEwyQuU0AAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Untitled\"\n        title=\"Untitled\"\n        src=\"/static/20f7320deea786acfb62dc9676129816/267f6/Untitled5.png\"\n        srcset=\"/static/20f7320deea786acfb62dc9676129816/e9ff0/Untitled5.png 180w,\n/static/20f7320deea786acfb62dc9676129816/f21e7/Untitled5.png 360w,\n/static/20f7320deea786acfb62dc9676129816/267f6/Untitled5.png 513w\"\n        sizes=\"(max-width: 513px) 100vw, 513px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<ul>\n<li>데이터를 입력하고 연산을 거쳐 결과를 도출해내는 과정을 <strong>순전파(Forward Propagation)</strong> 이라고 합니다. 이때 모델이 <strong>예측한 값(Estimation)</strong> 과 해당 데이터의 <strong>정답(Ground Truth)</strong> 의 차이를 <strong>손실 함수(Loss Function)</strong> 를 사용하여 <strong>에러</strong>, <strong>손실(Loss)</strong> 을 계산합니다. 이 손실 값을 이용해서 현재 예측에 사용된 Weight , Bias 등 파라미터들이 얼마나 잘못되었는지 체크하고 거꾸로 돌아가서 손실을 더 낮추는 방향으로 업데이트하는 것이 역전파입니다.</li>\n<li><strong>역전파(Backward Propagation)</strong> 은 역방향으로 출력에서 입력 방향으로  손실의 <strong>미분 값(Gradient)</strong> 이 줄어드는 방향으로 업데이트 합니다. 해당 방법을 <strong>경사하강법(Gradient Descent)</strong> 라고 합니다. <strong>연쇄법칙(Chain rule)</strong> 을 사용해서 편미분을 보다 쉽게 계산하는 것을 수학적으로 좀 더 자세히 설명하고 싶은데 일단 기초 이론이므로 다음에 기회가 된다면 새로운 포스팅을 작성해보겠습니다.</li>\n<li><strong>학습률(Learning Rate)</strong> 라는 것도 존재하는데 이는 학습된 값을 얼만큼 적용하냐를 결정해주는 하이퍼파라미터입니다. 해당 변수가 너무 크다면 <strong>과적합(Overfitting)</strong> 문제가 발생해서 발산해버리는 문제가 발생할 수 있습니다. 반면 너무 작다면 학습이 지나치게 오래 걸리는 문제가 발생할 수 있으므로 적당한 값을 찾아서 설정해주어야 합니다.</li>\n</ul>\n<h3 id=\"6-마무리\" style=\"position:relative;\"><a href=\"#6-%EB%A7%88%EB%AC%B4%EB%A6%AC\" aria-label=\"6 마무리 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>6. 마무리</h3>\n<p>이상으로 딥 러닝 기초 이론 포스팅을 마치겠습니다. 항상 말씀드리지만 틀린 부분도 있고 빼먹은 부분도 많을 거라고 생각합니다😭 자주 체크하며 틀린 부분은 고쳐나가겠습니다. 감사합니다!</p>\n<div class=\"table-of-contents\">\n<ul>\n<li><a href=\"#0-%EB%93%A4%EC%96%B4%EA%B0%80%EA%B8%B0-%EC%A0%84\">0. 들어가기 전..</a></li>\n<li><a href=\"#1-%EB%89%B4%EB%9F%B0\">1. 뉴런</a></li>\n<li><a href=\"#2-perceptron\">2. Perceptron</a></li>\n<li><a href=\"#3-weight%EA%B0%80%EC%A4%91%EC%B9%98-bias%ED%8E%B8%ED%96%A5\">3. Weight(가중치), Bias(편향)</a></li>\n<li><a href=\"#4-%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5-%EB%B6%80%ED%9D%A5cnn\">4. 인공지능 부흥(CNN)</a></li>\n<li><a href=\"#5-%EC%8B%A0%EA%B2%BD%EB%A7%9D%EC%9D%98-%ED%95%99%EC%8A%B5\">5. 신경망의 학습</a></li>\n<li><a href=\"#6-%EB%A7%88%EB%AC%B4%EB%A6%AC\">6. 마무리</a></li>\n</ul>\n</div>","frontmatter":{"date":"January 20, 2022","title":"딥러닝 기초 이론 정리","categories":"Basic","author":"Yon_ninii","emoji":"🤿"},"fields":{"slug":"/Deep learning basic theory/"}},"prev":null,"site":{"siteMetadata":{"siteUrl":"https://yon-ninii.github.io","comments":{"utterances":{"repo":""}}}}},"pageContext":{"slug":"/VGGnet/","nextSlug":"/Deep learning basic theory/","prevSlug":""}},
    "staticQueryHashes": ["1073350324","1956554647","2938748437"]}