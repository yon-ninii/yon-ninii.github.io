{
    "componentChunkName": "component---src-templates-blog-template-js",
    "path": "/Introduction/",
    "result": {"data":{"cur":{"id":"83174d6b-53fc-5e6e-9830-c250541485fe","html":"<h2 id=\"새롭게-만들게-된-ai와-컴퓨터-비전에-대해-공부한-내용을-정리하는-블로그입니다\" style=\"position:relative;\"><a href=\"#%EC%83%88%EB%A1%AD%EA%B2%8C-%EB%A7%8C%EB%93%A4%EA%B2%8C-%EB%90%9C-ai%EC%99%80-%EC%BB%B4%ED%93%A8%ED%84%B0-%EB%B9%84%EC%A0%84%EC%97%90-%EB%8C%80%ED%95%B4-%EA%B3%B5%EB%B6%80%ED%95%9C-%EB%82%B4%EC%9A%A9%EC%9D%84-%EC%A0%95%EB%A6%AC%ED%95%98%EB%8A%94-%EB%B8%94%EB%A1%9C%EA%B7%B8%EC%9E%85%EB%8B%88%EB%8B%A4\" aria-label=\"새롭게 만들게 된 ai와 컴퓨터 비전에 대해 공부한 내용을 정리하는 블로그입니다 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>새롭게 만들게 된 A.I와 컴퓨터 비전에 대해 공부한 내용을 정리하는 블로그입니다.</h2>\n<p>첫 포스팅이라 뭔가 긴장이 됩니다. 그래도 열심히 가꿔볼 생각이니 잘 봐주시면 감사하겠습니다.\n또한 그저 학부생 수준의 공부이니 전문적인 내용은 많이 틀릴 수 있습니다. 부정적인 피드백들도 환영입니다!\n프로필에도 있지만 제 소개를 간단히 해보자면 인하대학교 정보통신공학과에 재학중이고 현재 학교에 있는 A.I 컴퓨터 비전 연구실에서 학부 연구생으로 공부중입니다. 여러 비전관련 논문을 읽으면서 어디에 정리를 하고 싶어서 블로그를 시작했습니다.\n주로 논문 리뷰를 할 예정이고 따로 시간이 된다면 PyTorch와 수학적인 강의 리뷰도 다뤄보겠습니다. 잘 부탁드립니다!</p>\n<p>(+) 추가로 블로그 제작은 <a href=\"https://www.zoomkoding.com/gatsby-github-blog/\">줌코딩님</a>의 게시글을 참고해서 만들었습니다.</p>","excerpt":"새롭게 만들게 된 A.I와 컴퓨터 비전에 대해 공부한 내용을 정리하는 블로그입니다. 첫 포스팅이라 뭔가 긴장이 됩니다. 그래도 열심히 가꿔볼 생각이니 잘 봐주시면 감사하겠습니다.\n또한 그저 학부생 수준의 공부이니 전문적인 내용은 많이 틀릴 수 있습니다. 부정적인 피드백들도 환영입니다!\n프로필에도 있지만 제 소개를 간단히 해보자면 인하대학교 정보통신공학과에 재학중이고 현재 학교에 있는 A.I 컴퓨터 비전 연구실에서 학부 연구생으로 공부중입니다. 여러 비전관련 논문을 읽으면서 어디에 정리를 하고 싶어서 블로그를 시작했습니다.\n주로 논문 리뷰를 할 예정이고 따로 시간이 된다면 PyTorch와 수학적인 강의 리뷰도 다뤄보겠습니다. 잘 부탁드립니다! (+) 추가로 블로그 제작은 줌코딩님의 게시글을 참고해서 만들었습니다.","frontmatter":{"date":"January 14, 2022","title":"블로그 오픈했습니다!","categories":"Blog","author":"Yon_ninii","emoji":"😋"},"fields":{"slug":"/Introduction/"}},"next":null,"prev":{"id":"275a3a76-e646-5355-b9a5-bbd0502c3614","html":"<p><strong>안녕하세요. 오늘은 드디어 첫번째 논문 리뷰 포스팅입니다! 오늘 리뷰할 논문은 바로 2014년 옥스포드 대학의 연구팀 VGG에 의해 발표된 “Very Deep Convolutional Networks for Large-Scale Image Recognition” 입니다!</strong></p>\n<p><a href=\"https://arxiv.org/abs/1409.1556\">https://arxiv.org/abs/1409.1556</a></p>\n<p><strong>당시 이미지넷 인식 대회에서 준우승을 한 획기적인 네트워크였습니다. VGGNet은 VGG-16과 VGG-19로 나뉘는데 이 숫자는 Layer의 개수를 의미합니다. 한 마디로 16계층과 19계층짜리 네트워크를 만든거라고 보면 됩니다. 이 논문이 중요하다고 생각하는 이유는 해당 논문의 발표 이후로 네트워크의 깊이가 얼마나 중요한지, 또한 어떻게 깊게 만드는지에 대한 인식을 하기 시작했다고 생각하기 때문입니다.</strong></p>\n<p><strong>자, 이제 리뷰를 시작해보겠습니다.</strong></p>\n<h3 id=\"0-들어가기-전\" style=\"position:relative;\"><a href=\"#0-%EB%93%A4%EC%96%B4%EA%B0%80%EA%B8%B0-%EC%A0%84\" aria-label=\"0 들어가기 전 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>0. 들어가기 전..</h3>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 56.666666666666664%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsTAAALEwEAmpwYAAAB0klEQVQoz32S627TQBBG8/7vAkhI/EAFBKrUohYRR6Fx0jiJvZv6fontNL6uD7JdKiiEkdZjz2qOv539JlVV0TQNSinath2zaqnr+rl2binVAh2+7yOlJAxDJgCOY7Nc6azX9xjGGrmXmOaOxeIOS1i8jE4pQHE8FXhhMta6bhA26R99pKGHmN7iGUuaZlRXVeWg5FeDUt3wLrwDK+Fj+jlvb6ynff4E9g3VKaOpir8V9bC+gxE43cVcaA4iOPL6qxhq6iVwaKSjbSrOxd4/YPsx+j7l42wEvjkH7Lrx73VdkiQReZ6THhLK0xHLjcmPj9waEZ/mDoaTcaHZiOCRV9f/AaonJfebLUmasto9EEYxlwsXbRuiy4SrpcvWzfg8txF+xrtv8h8zfFLn313BY8T77xLpxnyYCuY7n6WxQ99YrDYmmr4hygq0e8nalCxkfOZSADn9wjGQzAx7sMPasrE9n614QNgu8sFmbe6xXQfdMHE9n5m+eb6sAaiebaGQtjMcVVhbhLDY7wWu53BIYrI0RQoLyzLJ82yYdVPX1L+5orfY5PJGQ/uhQdcOc+zBvd+iOCIIAqIoIgjHHCcxySEhjmOKoqAsS4qy5HQ6Dd99/gnOsEb9n3g3qgAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Untitled\"\n        title=\"Untitled\"\n        src=\"/static/a17b63ce85089208d871ff9ed011e461/37523/Untitled.png\"\n        srcset=\"/static/a17b63ce85089208d871ff9ed011e461/e9ff0/Untitled.png 180w,\n/static/a17b63ce85089208d871ff9ed011e461/f21e7/Untitled.png 360w,\n/static/a17b63ce85089208d871ff9ed011e461/37523/Untitled.png 720w,\n/static/a17b63ce85089208d871ff9ed011e461/fd28b/Untitled.png 811w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<ul>\n<li>\n<p>위 그림은 네트워크 깊이와 ImageNet 에러율에 대한 그래프입니다.</p>\n</li>\n<li>\n<p>VGG 이전에 발표된 AlexNet과 같은 네트워크들을 8개의 계층밖에 가지지 못하면서 에러율도 높다는 것을 확인할 수 있는데요, 반면에 VGG부터 Layer의 수가 늘어나면서 에러율 또한 현저히 낮아진 것을 알 수 있습니다.</p>\n</li>\n<li>\n<p>이 다음에 리뷰할 예정인 ResNet부터 Layer의 수가 폭발적으로 증가한 것 또한 확인할 수 있습니다.</p>\n</li>\n</ul>\n<h3 id=\"1-네트워크의-깊이depth\" style=\"position:relative;\"><a href=\"#1-%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC%EC%9D%98-%EA%B9%8A%EC%9D%B4depth\" aria-label=\"1 네트워크의 깊이depth permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>1. 네트워크의 깊이(Depth)</h3>\n<ul>\n<li>\n<p>그러면 왜 그동안 네트워크의 깊이를 마냥 늘려가지 못했을까요? 이 의문에 대한 답은 <strong>Gradient Vanishing, Overfitting, Increasing train time</strong> 등의 이유가 있습니다.</p>\n</li>\n<li>\n<p><strong>Gradient Vanishing</strong>은  활성화 함수(Activation function)을 이용해서 기울기 기반 학습(Gradient based training)을 진행할 때 발생하는 문제입니다. 간단하게 설명하자면 역전파를 이용해서 학습시킬 때 파라미터 값의 변화량인 Gradient가 너무 적게 변화한다면 training이 정상적으로 수행되지 않은 상태에서 수렴해버리는 문제가 발생하고 이를 Gradient가 사라진다고 해서 Gradient Vanishing이라고 합니다.</p>\n</li>\n<li>\n<p><strong>Overfitting</strong>은 과적합이라는 뜻인데, 훈련 데이터 셋으로 과하게 학습되어 모델의 파라미터들이 학습 데이터 셋에 너무 들어맞는(?) 상태가 되어 새로운 데이터 셋인 테스트 데이터가 들어오면 제대로 동작하지 못하는 문제입니다. 주로 모델이 너무 복잡할 때 발생합니다.</p>\n</li>\n<li>\n<p><strong>Increasing train time</strong>은 간단하게 학습 시간이 너무 오래 걸린다는 뜻입니다.</p>\n</li>\n<li>\n<p>이러한 문제점들이 항상 있었고 해당 논문의 저자는 이 한계들만 뛰어넘는다면 더욱 정확한 모델을 만들 수 있겠다는 생각이 들었다고 합니다.</p>\n</li>\n</ul>\n<h3 id=\"2-vggnet의-구조\" style=\"position:relative;\"><a href=\"#2-vggnet%EC%9D%98-%EA%B5%AC%EC%A1%B0\" aria-label=\"2 vggnet의 구조 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2. VGGNet의 구조</h3>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 63.33333333333333%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAAAsTAAALEwEAmpwYAAAB+klEQVQ4y52T22sTURDG8wcLoiKCQvHBJwWFUlCiPoiKl6I10sakLTYlXZsaamlNTHpJu203JtmQ3ewlu5u9/SRbtk2jJcWBc85wDvPN982ZSYRhyMD6/T5BEHAktYnvBmfsX9YSg61UKvPuzUtEOSS73vxvsFPA58+STNy9T3Y7pHhs4vcdgvAMdJjxWMCVlTw3rt9kcvo7+UOXjLBPSzGjx2AANsQ2GEowuuKEiWtXr3D73hOyuz45YYcZocHOoYqs9E5SBu6lpMagiYk7t0i+X2K+KDM994vlQo23mV1m8yLVap18qYUotum0DaqigtrpYVtdunYX3dZQTQXTNs4YTj56wIfFLdKrdWYyFRaz6zydl1jKbZFe2OTF8hE/Vn9SLOwyt9Zka+OAbueYY1Viv1njd1ei3pZwbOeEYfLxFKlchbQg8TFTZV6okRFEynstdE1HUXVcLyDEj+TbjoVpWhBcIPnLbIpPX0ukCw1efS6zeWSCb+N57kjA+PpFkqcmH5J8vUCq3OfbdpueYdDrWed6MQ5QNA9F7WEYOpqmYxgGlmX9/SlrFZXUhn5Oxb/6T9rbo1at0Gg0EUURWZbRNC2aslOGSqdN5UCNakbgRdJGwWJftWU0S6HvuNi2HbEzTRPHcc4AB47vuQS+h+/7F47duCmJJf8B0u3UofrXrXEAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Untitled\"\n        title=\"Untitled\"\n        src=\"/static/096973cca401812f3ce8da407885ca07/37523/Untitled1.png\"\n        srcset=\"/static/096973cca401812f3ce8da407885ca07/e9ff0/Untitled1.png 180w,\n/static/096973cca401812f3ce8da407885ca07/f21e7/Untitled1.png 360w,\n/static/096973cca401812f3ce8da407885ca07/37523/Untitled1.png 720w,\n/static/096973cca401812f3ce8da407885ca07/ae694/Untitled1.png 850w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<ul>\n<li>해당 그림은 VGG-16의 구조입니다.</li>\n<li>VGG팀은 깊이가 주는 영향력을 알기 위해 kernel은 3x3, stride는 1로 고정해서 깊지만 더욱 간단한 모델을 설계했습니다.</li>\n<li>해당 kernel로 convolution을 진행한 후에는 2x2 Max pooling을 2의 stride로 진행합니다.</li>\n<li>그렇다면 왜 3x3 convolution을 사용했을까요? 그 이유는 Receptive Field와 Parameter 수에 있습니다.</li>\n</ul>\n<h3 id=\"3-why-3x3-convolution\" style=\"position:relative;\"><a href=\"#3-why-3x3-convolution\" aria-label=\"3 why 3x3 convolution permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>3. Why 3x3 Convolution?</h3>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 75.55555555555556%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAYAAADkmO9VAAAACXBIWXMAAAsTAAALEwEAmpwYAAAC2ElEQVQ4y51TS09TURCec9+P3tvSCwJGNBrUAIkQBW0RYqIGEEpVKpTykLcV2zQgLwVBTcqzUIWiKJAQY5CIsHTtjo0LYmJYuDKu/BfXzPXUxA0YJzmZe8+c+c7MfN+BytBSrGZg/Vl5MNEHAGkAYBDCoHcAtWyXTzlW5NOy3XVaxvF8CQB4VVFIMq5JHPwx/8jm8qPVna2bA+tjAGDHOBBWt8mCDgAqAEjVkdVw/cjWc9/QRtxdO+zCPZAcmuUBVFXkRPotQkX3i17/w3eD5cGFZgqoyyKPXp5rKqwY9+XkVPVv9g4sftpun/j41u1/UooVZh894lzudNVUFhxOBwAbxxLdKgYAUnADgXAxBOx2hcd2hR+Jmqbv897z+bdfX/X1rd3zRFaCRZ5wHlZTW5qr/1y61bPUfiETABRV5LTf3TFsGsfxKYRhUii4tSSetZvbbWB+7gF6oYJVY5LA83hG2p30YIznGOKgeRpIopqhyHqqIuuGKutOgZcMAHAiKa86XOz7SClhRE1VdafOS2oaw3BWR4rACubegyQxIm2Xsf5M00TH0goIx/IC3mzYeCuuGlkg2ZyEMIwMhPBJQk3zK1zOS4e/rNk7VNjhe3yu9cZwYbN3yO2/1nMK54eg5s6adYZnCfyzhQJTi/db5uciTfFYf1ti8U7t0xCyzXGCo+XSaS5aV4BoROAYyx8IGA7MxEZDK9MTg2/igx0vZ4N10S5kW5VklA4y6DBsokJnpNN57QPYEJsdC69Oz41uLCBgd/1EF00Wvkx5QruTVRcBgDukS076epT9ARtnZvtaF8YjjfFof1ti7q4/2pmUyV7Mm7M7WZVOdaZT4e8PWHK2+kxZcSC3vCSQW1bckOfKrziBibLA6eaHFvgWvw6pmojPyqCA7EFj1KhcFPp2VUIIJtqCV04yKKksQwFCqMYOMruWKhqOTEGRbDaRl1DcOHTUIQf/Yb8AY5aW5OLuy9UAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Untitled\"\n        title=\"Untitled\"\n        src=\"/static/ee307bb02692b98b367df445aefa581b/37523/Untitled2.png\"\n        srcset=\"/static/ee307bb02692b98b367df445aefa581b/e9ff0/Untitled2.png 180w,\n/static/ee307bb02692b98b367df445aefa581b/f21e7/Untitled2.png 360w,\n/static/ee307bb02692b98b367df445aefa581b/37523/Untitled2.png 720w,\n/static/ee307bb02692b98b367df445aefa581b/302a4/Untitled2.png 1080w,\n/static/ee307bb02692b98b367df445aefa581b/21b4d/Untitled2.png 1280w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<ul>\n<li>\n<p>그림을 보면 쉽게 설명이 될 것 같습니다. 위 그림과 같이 7x7 입력 이미지에 3x3 convolution을 두번 진행하면 Receptive field는 5x5가 됩니다. 또한 아래 그림과 같이 5x5 convolution을 한번 진행하면 똑같이 5x5의 Receptive field를 가지게 됩니다. 하지만 이때 필요한 parameter 수가 바뀌는데요, 3x3을 두번 진행하면 9 + 9로 18개의 parameter가 필요하고 5x5는 25개의 parameter가 필요하게 됩니다. 즉, 같은 receptive field를 갖지만 적은 parameter가 쓰이는 네트워크를 만들 수 있다는 뜻입니다.</p>\n</li>\n<li>\n<p>고로 3x3 convolution만을 사용해서 네트워크를 설계하면 엄청나게 적어진 parameter 수로 앞서 설명한 문제점들이 줄어들었다는게 저자의 설명입니다.</p>\n</li>\n</ul>\n<h3 id=\"4-activation-function\" style=\"position:relative;\"><a href=\"#4-activation-function\" aria-label=\"4 activation function permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>4. Activation Function</h3>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 333px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 45.55555555555556%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAAAsTAAALEwEAmpwYAAAA+UlEQVQoz41Ra0/DMAzs//99CCS+MWBjYlMfZGni2L7JSVMy0B4nWRe78eVcd7gCVc1hCJFAidc68PtNVOFJwVLyDjdQm0yMRZp6YRN53QsGr4jpjmDr0PkZlFLOi5PCLzuGC3rR85DgHAnMsjhTSBYTuCDr2PVu1zZW/u8wLP9QkVjxvGWcYnEqTa8dV8FrkR0GgorApjVnP4uzPP6f+zeXYg4MiSICcRbzsW75cqKKbnMU7CfB6BW917yxGpaPM/A9JTx9ECbPgErZOgvSwiU4593XKCih2PaMzYHw2TPej4TdILn2diAMLoKIECjBnWbEhu0BopTPZywpxWqHo/RZAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Untitled\"\n        title=\"Untitled\"\n        src=\"/static/c9d2f71e8adb2ca57fa907f86a03d3d1/24c7e/Untitled3.png\"\n        srcset=\"/static/c9d2f71e8adb2ca57fa907f86a03d3d1/e9ff0/Untitled3.png 180w,\n/static/c9d2f71e8adb2ca57fa907f86a03d3d1/24c7e/Untitled3.png 333w\"\n        sizes=\"(max-width: 333px) 100vw, 333px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<ul>\n<li>\n<p>3x3 Convolution을 사용하는 또 하나의 장점은 convolution layer가 늘어남으로써 Activation Function을 더욱 여러번 수행할 수 있었습니다.</p>\n</li>\n<li>\n<p>네트워크를 깊게 만드는 데에서 <strong>비선형성(Non-linearity)</strong> 는 중요한 요소이기에 non-linear activation function인 <strong>ReLU</strong>를 사용해서 비선형성을 늘려줬다고 합니다.</p>\n</li>\n<li>\n<p>이렇게 비선형성을 늘려주면 모델은 더욱 유용한 특성을 뽑게 되고 학습의 효과가 증폭된다고 합니다.</p>\n</li>\n</ul>\n<h3 id=\"5-result\" style=\"position:relative;\"><a href=\"#5-result\" aria-label=\"5 result permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>5. Result</h3>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 102.22222222222221%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAACXBIWXMAAAsTAAALEwEAmpwYAAACe0lEQVQ4y33Uh25iQQwF0Pf/P4UUIdIghTRKQjrpBVJIIfHqOBoWrVZBssYz47Hvvfajur+/j4ODgxgOh3F7exubm5vRarXi8fExjo6OYnd3N25ubuLk5CT29vZydbe/vx+Hh4fx8PAQZ2dnMRgM8n11dXWVCSR1ubOzk0nu7u6i3+/HyspKBkq0vLycD4FYX1/PWL64tbW1uL6+/kko6OLiIh9Cyt/e3s5CGxsb0ev1YnV1Ner1eq6QlTiFLy8v4/z8PJlUKriwjsfjRNTtdpOWqgsLC3mPAVSFmmLisMKo3W5nXKWCgJKUbsyZygUdxGhDIs56enqaqLDkZ0KPwHdYLhVxtri4mIg1AAKmMZKRgoYSFd2xqFBjEIHPl1By3YNQ0q2trdxLVhjZawQQzhMhHVBSHSW6Mb4R4h8fH6cvzgp9s9nMvaTQQUzf6vPzM6bTaby9vaV9fX3l/vX1NT4+PnLvR1fjAQCEVomg03WFxFQeSvT8/BxPT0/pO9Px9/f3GI1G+VAyZ8XEM9K4p6PEFSHLrNFBNwWgwldZUzzodDopvEbyoYQQMufeV/MjoynzfmkACRRdWlqaNacMueT0N6N0Tw396IWq3/f3d9IpPloevry8pASk4RfK0GKjeRUU9LExAjrFitjmTqehEKPDxsr8MQxQLp/jLKFLCWkBuoTlHwUiuqJl9dCnZw956X5qqJNGo1Cmlz06zlB2VlAohqbCzmhOy0aj8fdLgYA2rIyFc0mhFoyygrQr4zWZTGZfGcu/r/mZmjePPEBPR2u12qwhzDurWEUUc1+Vjv1rAqFQ1R+rkfktVkJoqzJz/zMB9CI4fX6LLfYH8bS/pUwHwDsAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Untitled\"\n        title=\"Untitled\"\n        src=\"/static/38fbef97213d5711084b6187bcb04d34/37523/Untitled4.png\"\n        srcset=\"/static/38fbef97213d5711084b6187bcb04d34/e9ff0/Untitled4.png 180w,\n/static/38fbef97213d5711084b6187bcb04d34/f21e7/Untitled4.png 360w,\n/static/38fbef97213d5711084b6187bcb04d34/37523/Untitled4.png 720w,\n/static/38fbef97213d5711084b6187bcb04d34/302a4/Untitled4.png 1080w,\n/static/38fbef97213d5711084b6187bcb04d34/05fb0/Untitled4.png 1138w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<ul>\n<li>위 그림이 해당 논문에서 저자가 실험을 진행한 모델들입니다. A~E까지 6개의 모델로 나눠서 진행했는데 여기서 D와 E가 현재 우리가 아는 VGG-16과 19입니다.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 15.555555555555555%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAADCAYAAACTWi8uAAAACXBIWXMAAAsTAAALEwEAmpwYAAAApklEQVQI1zWPywqFAAhE+/8/qm0FQdG7iN4FRQ9atWhV56JwBRF1ZhyN932ROM+TLMvYto2qqnAchzzPqeuaJEk0fd/X3TAMmkVR0LYtz/PwfR+iZfR9r+AgCBjHkeu6VFQITdMQhiGmaeoBEZNecGVZ0nUdURRxHAfTNHHfN0aapliWhW3b6sh1XTzPQ+bLsrCuK3Ecs+878zzrF2JCsH8zUoUjmB/qx9VtYZw5HgAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Untitled\"\n        title=\"Untitled\"\n        src=\"/static/e8e1dd68e643f59eeea0efd68e6cd679/37523/Untitled5.png\"\n        srcset=\"/static/e8e1dd68e643f59eeea0efd68e6cd679/e9ff0/Untitled5.png 180w,\n/static/e8e1dd68e643f59eeea0efd68e6cd679/f21e7/Untitled5.png 360w,\n/static/e8e1dd68e643f59eeea0efd68e6cd679/37523/Untitled5.png 720w,\n/static/e8e1dd68e643f59eeea0efd68e6cd679/f79fa/Untitled5.png 940w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<ul>\n<li>위 표는 해당 모델들에서 사용된 Parameter의 수입니다. Layer의 수는 증가되었지만 parameter는 크게 증가되지 않은 점을 확인할 수 있습니다.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 44.44444444444444%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAAAsTAAALEwEAmpwYAAABd0lEQVQoz02S6Y6CQBCEef+30sguHtFIvFCjeIGo8RbBqzZfJ2z8MZkJ01X9dQ3Ofr9XkiRij6JI3o+rMBjo1/NUqVRUKpVULpdVr9fVbrfleZ5c11WtVlOj0dB2u9XxeNRut7PdieNYm83GzKI4VhJHOiSJ2r5vJoPBQNVqVePx2MQsagFAB8xqtVLh44xGI81mMz0eD2VZpizPld7vWi6XZna73dTtdk2Q57mZYU794XCwPQgCnU4npWkqJwxDw71er7pcLv/7fD63Ec/ns5rNphaLhZlPJhP1+30TQ0t9r9ezM7UO4s/no+fzaev1ehkpIowYyfd9I4YCAIjQcYfJcDg0Q4gdgmSUYjEC3SmCkFw6nY4RUgs5zTAqDImGsxnSFcL3+22roCXbVqtlRYy0Xq+NnAwhpDEN+AY1xETi8FpcfhNShIhfg3teGzIE0+nUyL8zZALOwFmGGNkLZ5mdETIyZLwuJhAiYPQiw+IhmIZo+A//AHZQm8Px+To+AAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Untitled\"\n        title=\"Untitled\"\n        src=\"/static/380d3159a4be8325c2f560f292c4db54/37523/Untitled6.png\"\n        srcset=\"/static/380d3159a4be8325c2f560f292c4db54/e9ff0/Untitled6.png 180w,\n/static/380d3159a4be8325c2f560f292c4db54/f21e7/Untitled6.png 360w,\n/static/380d3159a4be8325c2f560f292c4db54/37523/Untitled6.png 720w,\n/static/380d3159a4be8325c2f560f292c4db54/302a4/Untitled6.png 1080w,\n/static/380d3159a4be8325c2f560f292c4db54/37048/Untitled6.png 1352w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<ul>\n<li>하나의 Image scale만으로 테스트를 진행한 결과입니다.</li>\n<li>Error rate면에서 D와 E 모델이 나머지 모델들보다 우수한 것을 확인할 수 있습니다.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 720px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 36.11111111111111%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAYAAAAIy204AAAACXBIWXMAAAsTAAALEwEAmpwYAAABOUlEQVQoz1WR54rCABCE8/7v5B8rFmyXYiKKnphYYxJF7HN8e3hygWFJdmZ2duOkaarFYqE4jhUnib66XX1HkVzPV7lcVqvVUqVSURiGBt5d11W/39d4PNZms9F6vVaSJMqyTM5yuRSmbxR5rixNFYxGCoJA8/lcjUZDg8FARVEY0JxOJzMABDocDr+G+/1et9tNj8dD9/vdADkMR5rNZpaChPV6XbvdTqvVSr7vK89zbbdbGxBF0ceQxvV6/QPmNIbDoa0IuVqtqtlsmiErkhwOhlSGYggcPjyfT71eL6s85/PZRNPpVJPJRLVaTe1229KwEfeDg5bK0OPxaHC42+Vy+ZcQIQkhkrJUKtnaJOAHdDodE6PlPAxnU+AgxoTbvW9JAxLpSNnr9eR5nhlgyF+GQ1oqPHoY/gB0MgT1KSe1HQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Untitled\"\n        title=\"Untitled\"\n        src=\"/static/e9119b90d6146c0f28d9d236aa84694d/37523/Untitled7.png\"\n        srcset=\"/static/e9119b90d6146c0f28d9d236aa84694d/e9ff0/Untitled7.png 180w,\n/static/e9119b90d6146c0f28d9d236aa84694d/f21e7/Untitled7.png 360w,\n/static/e9119b90d6146c0f28d9d236aa84694d/37523/Untitled7.png 720w,\n/static/e9119b90d6146c0f28d9d236aa84694d/302a4/Untitled7.png 1080w,\n/static/e9119b90d6146c0f28d9d236aa84694d/85e74/Untitled7.png 1436w\"\n        sizes=\"(max-width: 720px) 100vw, 720px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<ul>\n<li>다음은 여러 scale에서 진행한 테스트 결과입니다.</li>\n<li>마찬가지로 D와 E 모델이 나머지 모델들보다 훨씬 우수하다는 것을 확인할 수 있습니다.</li>\n</ul>\n<h3 id=\"6-마무리\" style=\"position:relative;\"><a href=\"#6-%EB%A7%88%EB%AC%B4%EB%A6%AC\" aria-label=\"6 마무리 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>6. 마무리</h3>\n<p>해당 VGG 논문은 섣불리 늘리지 못했던 네트워크의 깊이를 늘린 것과 그에 따른 정확도의 향상의 상관관계를 널리 알린 연구로 큰 의미가 있다고 생각합니다. 이 다음엔 ResNet에 대한 논문을 리뷰해볼 예정인데 폭발적으로 증가한 Layer의 개수는 또 어떤 방법을 사용해서 늘린건지 한번 정리를 해보도록 하겠습니다 😄</p>\n<p>읽어주셔서 감사합니다!</p>\n<div class=\"table-of-contents\">\n<ul>\n<li><a href=\"#0-%EB%93%A4%EC%96%B4%EA%B0%80%EA%B8%B0-%EC%A0%84\">0. 들어가기 전..</a></li>\n<li><a href=\"#1-%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC%EC%9D%98-%EA%B9%8A%EC%9D%B4depth\">1. 네트워크의 깊이(Depth)</a></li>\n<li><a href=\"#2-vggnet%EC%9D%98-%EA%B5%AC%EC%A1%B0\">2. VGGNet의 구조</a></li>\n<li><a href=\"#3-why-3x3-convolution\">3. Why 3x3 Convolution?</a></li>\n<li><a href=\"#4-activation-function\">4. Activation Function</a></li>\n<li><a href=\"#5-result\">5. Result</a></li>\n<li><a href=\"#6-%EB%A7%88%EB%AC%B4%EB%A6%AC\">6. 마무리</a></li>\n</ul>\n</div>","frontmatter":{"date":"January 18, 2022","title":"딥러닝 기초 용어 정리","categories":"Basic","author":"Yon_ninii","emoji":"🧚🏻‍♀️"},"fields":{"slug":"/VGGnet/"}},"site":{"siteMetadata":{"siteUrl":"https://yon-ninii.github.io","comments":{"utterances":{"repo":""}}}}},"pageContext":{"slug":"/Introduction/","nextSlug":"","prevSlug":"/VGGnet/"}},
    "staticQueryHashes": ["1073350324","1956554647","2938748437"]}